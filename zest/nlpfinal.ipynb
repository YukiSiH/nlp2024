{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-07T02:41:27.765047Z",
     "iopub.status.busy": "2024-11-07T02:41:27.764660Z",
     "iopub.status.idle": "2024-11-07T02:41:29.875277Z",
     "shell.execute_reply": "2024-11-07T02:41:29.874273Z",
     "shell.execute_reply.started": "2024-11-07T02:41:27.765003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import zstandard as zstd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Union, Dict, List\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from matplotlib import font_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T02:41:34.676819Z",
     "iopub.status.busy": "2024-11-07T02:41:34.676293Z",
     "iopub.status.idle": "2024-11-07T02:41:34.681727Z",
     "shell.execute_reply": "2024-11-07T02:41:34.680800Z",
     "shell.execute_reply.started": "2024-11-07T02:41:34.676766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 对输入字符串进行分词处理，将字符串转化为小写并用非单词字符进行分割\n",
    "def tokenize(s: str) -> list:\n",
    "    return re.split(r'\\W+', s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T02:41:35.663271Z",
     "iopub.status.busy": "2024-11-07T02:41:35.662602Z",
     "iopub.status.idle": "2024-11-07T02:41:35.668162Z",
     "shell.execute_reply": "2024-11-07T02:41:35.667150Z",
     "shell.execute_reply.started": "2024-11-07T02:41:35.663228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 规范化输入字符串，移除多余的空白和非单词字符\n",
    "def normalize(s: str) -> str:\n",
    "    s0 = re.sub(r'\\b\\W+ | \\W+\\b', ' ', s.lower())\n",
    "    return re.sub(r'\\b[^\\w\\s]+\\b', '', s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T02:41:36.161905Z",
     "iopub.status.busy": "2024-11-07T02:41:36.161503Z",
     "iopub.status.idle": "2024-11-07T02:41:36.172385Z",
     "shell.execute_reply": "2024-11-07T02:41:36.171455Z",
     "shell.execute_reply.started": "2024-11-07T02:41:36.161868Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 字符串规范化类\n",
    "class StringNormalizer:\n",
    "    def __init__(self, no_punct: bool = False, keep_punct: bool = False):\n",
    "        self.patterns_to_replace = []\n",
    "        # 根据参数决定是否去除标点符号\n",
    "        if no_punct:\n",
    "            self.patterns_to_replace += [('[¿?#!¡_]+|\\.\\.\\.+', ' ')]\n",
    "        else:\n",
    "            if keep_punct is False:\n",
    "                self.patterns_to_replace += [\n",
    "                    ('\\.\\.\\.+', ' ___ '),  # 替换省略号\n",
    "                    ('[¿?]+', ' _?_ '),    # 替换问号\n",
    "                    ('[!¡]+', ' _!_ '),    # 替换感叹号\n",
    "                    ('#', \" _#_ \"),        # 替换井号\n",
    "                ]\n",
    "        \n",
    "        # 定义其他替换模式\n",
    "        self.patterns_to_replace += [\n",
    "            ('[^\\w\\d?!_*&|#]+', ' '),  # 移除非单词字符\n",
    "            (\"(?<=\\w)[’']+(?=\\w)|(?<=\\d)[,._\\s]+(?=\\d)\", ''),  # 处理数字间的标点\n",
    "            ('(?<!\\d)\\d\\d?(?!\\d)', ' ## '),  # 替换小于三位的数字\n",
    "            ('(?<!\\d)\\d+(?!\\d)', ' ### '),  # 替换多位数字\n",
    "            ('\\s+', ' ')  # 替换多个空格为一个空格\n",
    "        ]\n",
    "\n",
    "    # 规范化字符串\n",
    "    def normalize_str(self, text: str):\n",
    "        t = f' {text} '  # 在字符串开头和结尾添加空格以便后续处理\n",
    "        for p in self.patterns_to_replace:\n",
    "            t = re.sub(p[0], p[1], t)  # 应用所有替换模式\n",
    "        return t[1:-1]  # 返回处理后的字符串，去除前后的空格\n",
    "\n",
    "    # 从规范化字符串中提取单词\n",
    "    def get_words(self, text: str):\n",
    "        norm_text = self.normalize_str(text)\n",
    "        return re.findall('\\W*(\\w+)', norm_text)  # 使用正则表达式提取单词\n",
    "\n",
    "    # 获取单词计数\n",
    "    def get_word_count(self, text: str):\n",
    "        return len(self.get_words(text))  # 返回单词数量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T02:41:36.661516Z",
     "iopub.status.busy": "2024-11-07T02:41:36.660709Z",
     "iopub.status.idle": "2024-11-07T02:41:36.680964Z",
     "shell.execute_reply": "2024-11-07T02:41:36.679941Z",
     "shell.execute_reply.started": "2024-11-07T02:41:36.661475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 字符计数类\n",
    "class CharCounter:\n",
    "    def __init__(self,\n",
    "                 samples: list,\n",
    "                 name='',\n",
    "                 IDF_thresh: float = 0.005,\n",
    "                 max_samples: int = 50000):\n",
    "        self.name = name\n",
    "        self.IDF_thresh = IDF_thresh\n",
    "        self.num_samples = min(max_samples, len(samples))  # 限制样本数\n",
    "        self.populate_char_ctrs(samples[:max_samples], IDF_thresh)  # 填充字符计数器\n",
    "\n",
    "        print(f\"{len(self.good_chars)} good characters: \", self.good_chars)\n",
    "        print(f\"{len(self.leftover_chars)} leftover characters: \", self.leftover_chars)\n",
    "\n",
    "        prob = self.compute_total_char_prob()\n",
    "        if prob < 0.99:\n",
    "            print(\"WARNING: In CharCounter, total probability is\", prob, \"(should be 1.0)\")\n",
    "\n",
    "    # 返回字符计数器的字符串表示\n",
    "    def __str__(self):\n",
    "        return (f\"Character counter for *{self.name}* based on {self.num_samples} samples\"\n",
    "                f\" and {self.num_chars} characters\"\n",
    "                f\"\\nTop characters by DF with threshold {100 * self.IDF_thresh:.3}% \"\n",
    "                f\"(blank + word characters):\\n {self.ctr_df_trimmed}\"\n",
    "                f\"\\nTop characters by occurrence:\\n {self.ctr_occ_trimmed}\"\n",
    "                f\"\\nLeftover characters: {self.leftover_chars}\")\n",
    "\n",
    "    # 获取字符计数\n",
    "    @staticmethod\n",
    "    def get_char_counts(titles: list):\n",
    "        ctr_df = Counter()  # 计数器，记录字符的出现频率\n",
    "        ctr_occ = Counter()  # 计数器，记录字符的总出现次数\n",
    "        OK_chars = re.compile('[^\\w\\d .,:;¿?¡!_*+/|#]+')  # 允许的字符正则表达式\n",
    "        for t in titles:\n",
    "            try:\n",
    "                tn = re.sub(OK_chars, '', t)  # 移除不允许的字符\n",
    "            except TypeError:\n",
    "                print(t)\n",
    "                print(type(t))\n",
    "                return\n",
    "            ctr_df += Counter(set(tn))  # 统计不重复字符\n",
    "            ctr_occ += Counter(tn)  # 统计字符的出现次数\n",
    "        return ctr_df, ctr_occ\n",
    "\n",
    "    # 填充字符计数器\n",
    "    def populate_char_ctrs(self, samples: list, IDF_thresh: float):\n",
    "        n = len(samples)\n",
    "        self.ctr_df, self.ctr_occ = self.get_char_counts(samples)  # 获取字符计数\n",
    "        print(\"Found\", len(self.ctr_df), \"different characters (words, punctuation, blanks), not including emojis\")\n",
    "        self.ctr_df_trimmed = Counter()  # 修剪后的字符计数器\n",
    "        self.ctr_occ_trimmed = Counter()  # 修剪后的字符出现次数计数器\n",
    "        for chr, count in self.ctr_df.most_common(int(10 / IDF_thresh)):  # 根据阈值修剪字符\n",
    "            self.ctr_df_trimmed[chr] = count\n",
    "            self.ctr_occ_trimmed[chr] = self.ctr_occ[chr]\n",
    "            self.min_occ = self.ctr_occ[chr]\n",
    "            if count < n * IDF_thresh: break\n",
    "        self.num_chars = sum(self.ctr_occ_trimmed.values())  # 计算总字符数\n",
    "        self.good_chars = ''.join([c for c in self.ctr_df_trimmed])  # 好字符集合\n",
    "        self.leftover_chars = ''.join([c for c in self.ctr_df - self.ctr_df_trimmed])  # 剩余字符集合\n",
    "        self.bad_chars_regex = re.compile('[^' + self.good_chars + ']+')  # 不好的字符正则表达式\n",
    "\n",
    "    # 移除不好的字符\n",
    "    def remove_bad_chars(self, s: str):\n",
    "        return re.sub('\\s+', ' ', re.sub(self.bad_chars_regex, '', s))\n",
    "\n",
    "    # 计算交叉熵\n",
    "    def compute_cross_entropy(self, s: str):\n",
    "        if len(s) == 0: return 0.0\n",
    "        sum_lp = 0\n",
    "        for c in s:\n",
    "            p = self.get_char_prob(c)\n",
    "            sum_lp += math.log(p)\n",
    "        return -sum_lp / len(s)\n",
    "\n",
    "    # 计算基于好字符的交叉熵\n",
    "    def compute_cross_entropy1(self, s: str):\n",
    "        s1 = self.remove_bad_chars(s)  # 移除不好的字符\n",
    "        if len(s1) == 0: return 0.0\n",
    "        sum_lp = 0\n",
    "        for c in s1:\n",
    "            p = self.get_char_prob(c)\n",
    "            sum_lp += math.log(p)\n",
    "        return -sum_lp / len(s1)\n",
    "\n",
    "    # 获取字符的概率\n",
    "    def get_char_prob(self, c: str):\n",
    "        count = self.ctr_occ_trimmed[c]  # 获取字符出现次数\n",
    "        if count == 0:\n",
    "            count = self.min_occ / 2.0  # 若字符未出现，则使用最小出现次数的一半\n",
    "        return count / self.num_chars  # 返回字符概率\n",
    "\n",
    "    # 计算字符的总概率\n",
    "    def compute_total_char_prob(self):\n",
    "        sum = 0.0\n",
    "        for c in self.ctr_occ_trimmed:\n",
    "            sum += self.get_char_prob(c)\n",
    "        return sum  # 返回字符总概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T02:41:37.172864Z",
     "iopub.status.busy": "2024-11-07T02:41:37.172007Z",
     "iopub.status.idle": "2024-11-07T02:41:37.250174Z",
     "shell.execute_reply": "2024-11-07T02:41:37.249051Z",
     "shell.execute_reply.started": "2024-11-07T02:41:37.172823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 单词计数类\n",
    "class WordCounter:\n",
    "    def __init__(self,\n",
    "                 samples: list,\n",
    "                 name='',\n",
    "                 IDF_thresh: float = 0.0001,\n",
    "                 max_samples: int = 50000):\n",
    "        self.name = name\n",
    "        self.IDF_thresh = IDF_thresh\n",
    "        self.num_samples = min(max_samples, len(samples))  # 限制样本数\n",
    "        self.populate_word_ctrs(samples[:max_samples], IDF_thresh)  # 填充单词计数器\n",
    "\n",
    "        prob = self.compute_total_word_prob()\n",
    "        if prob < 0.99:\n",
    "            print(\"WARNING: In WordCounter, total probability is\", prob, \"(should be 1.0)\")\n",
    "\n",
    "    # 返回单词计数器的字符串表示\n",
    "    def __str__(self):\n",
    "        return (f\"Word counter for *{self.name}* based on {self.num_samples} samples\"\n",
    "                f\" and {self.num_words} words\"\n",
    "                f\"\\nTop words by DF with threshold {100 * self.IDF_thresh:.3}% : {self.ctr_df_trimmed}\"\n",
    "                f\"\\nTop words by occurrence:\\n {self.ctr_occ_trimmed}\")\n",
    "\n",
    "    # 获取单词计数\n",
    "    @staticmethod\n",
    "    def get_word_counts(titles: list):\n",
    "        ctr_df = Counter()  # 计数器，记录单词的出现频率\n",
    "        ctr_occ = Counter()  # 计数器，记录单词的总出现次数\n",
    "        for t in titles:\n",
    "            words = t.split()  # 按空格分割单词\n",
    "            ctr_df += Counter(set(words))  # 统计不重复单词\n",
    "            ctr_occ += Counter(words)  # 统计单词的出现次数\n",
    "        return ctr_df, ctr_occ\n",
    "\n",
    "    # 填充单词计数器\n",
    "    def populate_word_ctrs(self, samples: list, IDF_thresh: float):\n",
    "        n = len(samples)\n",
    "        self.ctr_df, self.ctr_occ = self.get_word_counts(samples)  # 获取单词计数\n",
    "        print(\"Found\", len(self.ctr_df), \"different words\")\n",
    "        self.ctr_df_trimmed = Counter()  # 修剪后的单词计数器\n",
    "        self.ctr_occ_trimmed = Counter()  # 修剪后的单词出现次数计数器\n",
    "        for w, count in self.ctr_df.most_common(int(10 / IDF_thresh)):  # 根据阈值修剪单词\n",
    "            self.ctr_df_trimmed[w] = count\n",
    "            self.ctr_occ_trimmed[w] = self.ctr_occ[w]\n",
    "            self.min_occ = self.ctr_occ[w]\n",
    "            if count < n * IDF_thresh: break\n",
    "        self.num_words = sum(self.ctr_df_trimmed.values())  # 计算总单词数\n",
    "        print(f\"Trimmed down to {len(self.ctr_df_trimmed)} words\")\n",
    "\n",
    "    # 移除罕见单词\n",
    "    def remove_rare_words(self, s: str):\n",
    "        words = s.split()\n",
    "        new_words = [w for w in words if self.ctr_df_trimmed[w] > 0]  # 保留出现次数大于0的单词\n",
    "        return ' '.join(new_words)  # 返回新字符串\n",
    "\n",
    "    # 计算单词的交叉熵\n",
    "    def compute_cross_entropy(self, s: str):\n",
    "        if len(s) == 0: return 0.0\n",
    "        words = s.split()\n",
    "        sum_lp = 0\n",
    "        for c in words:\n",
    "            p = self.get_word_prob(c)\n",
    "            sum_lp += math.log(p)\n",
    "        return -sum_lp / len(words)\n",
    "\n",
    "    # 计算基于频繁单词的交叉熵\n",
    "    def compute_cross_entropy1(self, s: str):\n",
    "        if len(s) == 0: return 0.0\n",
    "        words = s.split()\n",
    "        new_words = [w for w in words if self.ctr_df_trimmed[w] > 0]  # 保留出现次数大于0的单词\n",
    "        if len(new_words) == 0: return 0.0\n",
    "        sum_lp = 0\n",
    "        for c in new_words:\n",
    "            p = self.get_word_prob(c)\n",
    "            sum_lp += math.log(p)\n",
    "        return -sum_lp / len(new_words)\n",
    "\n",
    "    # 获取单词的概率\n",
    "    def get_word_prob(self, word: str):\n",
    "        count = self.ctr_occ_trimmed[word]  # 获取单词出现次数\n",
    "        if count == 0:\n",
    "            count = self.min_occ / 2  # 若单词未出现，则使用最小出现次数的一半\n",
    "        return count / self.num_words  # 返回单词概率\n",
    "\n",
    "    # 计算单词的总概率\n",
    "    def compute_total_word_prob(self):\n",
    "        sum = 0.0\n",
    "        for word in self.ctr_occ_trimmed:\n",
    "            sum += self.get_word_prob(word)\n",
    "        return sum  # 返回单词总概率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T02:41:37.791500Z",
     "iopub.status.busy": "2024-11-07T02:41:37.791113Z",
     "iopub.status.idle": "2024-11-07T02:41:37.796490Z",
     "shell.execute_reply": "2024-11-07T02:41:37.795563Z",
     "shell.execute_reply.started": "2024-11-07T02:41:37.791461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 清理字符串，使用字符串规范化器和字符计数器\n",
    "def cleanup_str(s: str, string_normalizer=None, char_ctr=None):\n",
    "    return string_normalizer.normalize_str(char_ctr.remove_bad_chars(s))\n",
    "\n",
    "# 清理语料库，遍历每个样本\n",
    "def cleanup_corpus(corpus: list, string_normalizer=None, char_ctr=None):\n",
    "    return [cleanup_str(s, string_normalizer, char_ctr) for s in corpus]  # 返回清理后的语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T02:41:38.462393Z",
     "iopub.status.busy": "2024-11-07T02:41:38.462017Z",
     "iopub.status.idle": "2024-11-07T02:41:38.468717Z",
     "shell.execute_reply": "2024-11-07T02:41:38.467839Z",
     "shell.execute_reply.started": "2024-11-07T02:41:38.462357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_hashed_words(s: str):\n",
    "    \"\"\"\n",
    "    将输入字符串中的每个单词转换为其哈希值的字节表示。\n",
    "    该函数返回一个字节序列，每个单词用其哈希值的字节表示。\n",
    "    \n",
    "    :param s: 输入字符串\n",
    "    :return: 表示单词哈希值的字节序列\n",
    "    \"\"\"\n",
    "    bytes_list = []  # 用于存储每个单词的字节表示\n",
    "    for w in s.split():\n",
    "        if w == '': \n",
    "            continue  # 跳过空字符串\n",
    "        h = hash(w)  # 获取单词的哈希值\n",
    "        while h > 0:\n",
    "            # 将哈希值转换为字节，范围为1到255\n",
    "            bytes_list.append(1 + h % 255)  # 255而不是256，以便0可以作为单词分隔符\n",
    "            h = h // 255  # 更新哈希值，向下取整\n",
    "        bytes_list.append(0)  # 添加字节0作为结束符\n",
    "    return bytes(bytes_list)  # 返回字节序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T02:41:39.062192Z",
     "iopub.status.busy": "2024-11-07T02:41:39.061812Z",
     "iopub.status.idle": "2024-11-07T02:41:39.081275Z",
     "shell.execute_reply": "2024-11-07T02:41:39.080281Z",
     "shell.execute_reply.started": "2024-11-07T02:41:39.062154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ZstdLangModel:\n",
    "    \"\"\"\n",
    "    Zstandard语言模型，用于文本压缩。\n",
    "    假设输入已经过预处理（归一化等）。\n",
    "    \"\"\"\n",
    "\n",
    "    ZSTD_LEVEL = 22  # 压缩级别\n",
    "    dict_size_factor = 3 / 5  # 字典大小因子\n",
    "    verbose = 0  # 是否输出详细信息\n",
    "    max_num_dicts = 7  # 最大字典数量\n",
    "    min_dict_size = 150  # 最小字典大小（字节）\n",
    "\n",
    "    def __init__(self, name: str, corpus: list, primary_dict_size: int = 0, hash_words: bool = False):\n",
    "        \"\"\"\n",
    "        初始化Zstd语言模型。\n",
    "\n",
    "        :param name: 模型名称\n",
    "        :param corpus: 训练语料库\n",
    "        :param primary_dict_size: 主字典大小\n",
    "        :param hash_words: 是否对单词进行哈希处理\n",
    "        \"\"\"\n",
    "        if self.verbose > 0:\n",
    "            print(f'For the \"{name}\" corpus of size {len(corpus)}: ')\n",
    "            not_str = '' if hash_words else ' not'\n",
    "            print(f'  * words are{not_str} hashed ')  # 输出是否对单词进行哈希处理\n",
    "        self.name = name  # 保存模型名称\n",
    "        self.primary_dict_size = primary_dict_size  # 设置主字典大小\n",
    "        self.hash_words = hash_words  # 设置是否哈希单词\n",
    "\n",
    "        self.set_params(corpus)  # 设置参数\n",
    "        self.train_dicts(corpus)  # 训练字典\n",
    "\n",
    "    def set_params(self, corpus: list):\n",
    "        \"\"\"\n",
    "        设置模型参数，包括主字典大小和字典数量。\n",
    "\n",
    "        :param corpus: 训练语料库\n",
    "        \"\"\"\n",
    "        # 如果需要哈希单词，则生成字节对象列表\n",
    "        if self.hash_words:\n",
    "            self.bytes_obj_list = [get_hashed_words(s) + b'|' for s in corpus]\n",
    "            corpus_b = b''.join(self.bytes_obj_list)  # 连接所有字节\n",
    "        else:\n",
    "            corpus_b = bytes('\\n'.join(corpus), 'utf8')  # 将文本语料库转换为字节\n",
    "\n",
    "        orig_size = len(corpus_b)  # 记录原始大小\n",
    "        self.c = zstd.ZstdCompressor(level=self.ZSTD_LEVEL)  # 创建压缩器\n",
    "        compr_size = len(self.c.compress(corpus_b))  # 获取压缩后的大小\n",
    "        del corpus_b  # 删除临时字节对象以释放内存\n",
    "\n",
    "        # 如果未指定主字典大小，则根据压缩后的大小计算\n",
    "        if self.primary_dict_size == 0:\n",
    "            self.primary_dict_size = int(self.dict_size_factor * compr_size)\n",
    "\n",
    "        # 计算字典数量\n",
    "        self.num_dicts = max(min(\n",
    "            self.max_num_dicts,\n",
    "            int(math.log2(self.primary_dict_size / self.min_dict_size))), 1)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(\n",
    "                f'  * compression rate = {compr_size / orig_size:.4}, compr. size {compr_size}'\n",
    "            )  # 输出压缩率\n",
    "\n",
    "    def train_dicts(self, corpus: list):\n",
    "        \"\"\"\n",
    "        训练字典。\n",
    "\n",
    "        :param corpus: 训练语料库\n",
    "        \"\"\"\n",
    "        samples = self.bytes_obj_list if self.hash_words else [\n",
    "            bytes(s, 'utf8') for s in corpus  # 将语料库转换为字节格式\n",
    "        ]\n",
    "\n",
    "        def get_dict(j: int, primary_dict_size: int, samples: list):\n",
    "            \"\"\"\n",
    "            生成指定大小的训练字典。\n",
    "\n",
    "            :param j: 字典索引\n",
    "            :param primary_dict_size: 主字典大小\n",
    "            :param samples: 样本数据\n",
    "            :return: 训练生成的字典\n",
    "            \"\"\"\n",
    "            sz = max(int(primary_dict_size / 2**j), 256)  # 确定字典大小\n",
    "            return zstd.train_dictionary(sz, samples)  # 训练并返回字典\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f'  * building {self.num_dicts} dictionaries of size {self.primary_dict_size} and smaller'\n",
    "            )  # 输出正在构建的字典数量和大小\n",
    "\n",
    "        # 生成字典列表\n",
    "        self.dicts = [\n",
    "            get_dict(j, self.primary_dict_size, samples)\n",
    "            for j in range(self.num_dicts)\n",
    "        ]\n",
    "\n",
    "        # 创建压缩器上下文，忽略各种头部信息\n",
    "        self.cctx = [\n",
    "            zstd.ZstdCompressor(\n",
    "                dict_data=d,\n",
    "                write_content_size=False,\n",
    "                write_dict_id=False,\n",
    "                write_checksum=False,\n",
    "                level=self.ZSTD_LEVEL) for d in self.dicts\n",
    "        ]\n",
    "\n",
    "        self.header_size = len(self.cctx[0].compress(b''))  # 计算压缩头部大小\n",
    "        if self.verbose > 0:\n",
    "            print('  * compression headers of size', self.header_size,\n",
    "                  'will be neglected')  # 输出将忽略的头部大小\n",
    "\n",
    "        if self.hash_words: \n",
    "            del self.bytes_obj_list  # 删除字节对象列表以释放内存\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_compr_size(self, t: str):\n",
    "        \"\"\"\n",
    "        获取给定文本的压缩大小。\n",
    "\n",
    "        :param t: 输入文本\n",
    "        :return: 压缩大小和原始大小\n",
    "        \"\"\"\n",
    "        t_b = bytes(t, 'utf8')  # 将文本转换为字节\n",
    "        l_orig = len(t_b)  # 原始大小\n",
    "        if l_orig < 5: \n",
    "            return l_orig, l_orig  # 若文本长度小于5，则直接返回原始大小\n",
    "\n",
    "        if self.hash_words:\n",
    "            t_b = get_hashed_words(t)  # 对文本进行哈希处理\n",
    "\n",
    "        # 计算压缩后的大小\n",
    "        l_compr = sum([len(c.compress(t_b))\n",
    "                       for c in self.cctx]) / len(self.cctx) - self.header_size\n",
    "        l_orig = len(self.c.compress(t_b)) - self.header_size  # 计算原始压缩大小\n",
    "        return l_compr, l_orig  # 返回压缩大小和原始大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T02:41:39.565752Z",
     "iopub.status.busy": "2024-11-07T02:41:39.565131Z",
     "iopub.status.idle": "2024-11-07T02:41:39.574915Z",
     "shell.execute_reply": "2024-11-07T02:41:39.573967Z",
     "shell.execute_reply.started": "2024-11-07T02:41:39.565712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Zstd2Classifier:\n",
    "    \"\"\"\n",
    "    Zstandard分类器，使用ZstdLangModel进行分类。\n",
    "    假设输入已经过预处理（归一化等）。\n",
    "    \"\"\"\n",
    "\n",
    "    verbose = 0  # 是否输出详细信息\n",
    "\n",
    "    def __init__(self, name: str, pos_samples: list, neg_samples: list, hash_words: bool = False):\n",
    "        \"\"\"\n",
    "        初始化分类器。\n",
    "\n",
    "        :param name: 分类器名称\n",
    "        :param pos_samples: 正样本列表\n",
    "        :param neg_samples: 负样本列表\n",
    "        :param hash_words: 是否对单词进行哈希处理\n",
    "        \"\"\"\n",
    "        if self.verbose > 0:\n",
    "            print(\n",
    "                f'Building a \"{name}\" classifier for {len(pos_samples)} positive and '\n",
    "                f'{len(neg_samples)} negative samples')  # 输出构建分类器的信息\n",
    "\n",
    "        # 创建正负样本的Zstd语言模型\n",
    "        self.pos_model = ZstdLangModel('positive', pos_samples, 0, hash_words)\n",
    "        self.neg_model = ZstdLangModel('negative', neg_samples,\n",
    "                                       self.pos_model.primary_dict_size, hash_words)\n",
    "\n",
    "    def score(self, s: str, num_reps: int = 2):\n",
    "        \"\"\"\n",
    "        对给定字符串进行评分。\n",
    "\n",
    "        :param s: 输入字符串\n",
    "        :param num_reps: 字符串重复次数\n",
    "        :return: 分类得分、负样本压缩率、正样本压缩率\n",
    "        \"\"\"\n",
    "        s_x = s * num_reps  # 重复输入字符串\n",
    "        # 获取正负样本的压缩大小和原始大小\n",
    "        compr_sz_pos, orig_sz_pos = self.pos_model.get_compr_size(s_x)\n",
    "        compr_sz_neg, orig_sz_neg = self.neg_model.get_compr_size(s_x)\n",
    "        assert orig_sz_pos == orig_sz_neg, 'Two orig size estimates differ'  # 确保原始大小一致\n",
    "        if orig_sz_pos == 0:\n",
    "            return 0.0, 1.0, 1.0  # 若原始大小为0，返回默认值\n",
    "\n",
    "        # 计算正负样本的压缩率\n",
    "        pos_rate = compr_sz_pos / orig_sz_pos\n",
    "        neg_rate = compr_sz_neg / orig_sz_neg\n",
    "\n",
    "        score = neg_rate - pos_rate  # 计算分类得分\n",
    "        return score, neg_rate, pos_rate  # 返回得分和压缩率"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6015772,
     "sourceId": 9812754,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6027987,
     "sourceId": 9829007,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
